# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import plpy

from utilities.control import MinWarning
from utilities.utilities import _assert
from utilities.utilities import unique_string
from utilities.utilities import add_postfix
from utilities.utilities import NUMERIC, ONLY_ARRAY
from utilities.utilities import is_valid_psql_type
from utilities.utilities import is_platform_pg
from utilities.utilities import num_features
from utilities.utilities import get_seg_number
from utilities.validate_args import input_tbl_valid, output_tbl_valid
from utilities.validate_args import is_var_valid
from utilities.validate_args import cols_in_tbl_valid
from utilities.validate_args import get_expr_type
from utilities.validate_args import get_algorithm_name
from graph.wcc import wcc

from math import log
from math import floor
from math import sqrt

from datetime import datetime
from rtree import index
from rtree.index import Rtree

BRUTE_FORCE = 'brute_force'
KD_TREE = 'kd_tree'

def print_count(table):

    plpy.info("{0} count {1}".format(table, plpy.execute("select count(*) from {0}".format(table))[0]['count']))

def dbscan(schema_madlib, source_table, output_table, id_column, expr_point, eps, min_samples, metric, algorithm, depth, **kwargs):

    with MinWarning("warning"):

        min_samples = 5 if not min_samples else min_samples
        metric = 'squared_dist_norm2' if not metric else metric
        algorithm = 'brute' if not algorithm else algorithm
        depth = 3 if not depth else depth

        algorithm = get_algorithm_name(algorithm, BRUTE_FORCE,
            [BRUTE_FORCE, KD_TREE], 'DBSCAN')

        _validate_dbscan(schema_madlib, source_table, output_table, id_column,
                         expr_point, eps, min_samples, metric, algorithm, depth)

        dist_src_sql = ''  if is_platform_pg() else 'DISTRIBUTED BY (__src__)'
        dist_id_sql = ''  if is_platform_pg() else 'DISTRIBUTED BY ({0})'.format(id_column)
        dist_reach_sql = ''  if is_platform_pg() else 'DISTRIBUTED BY (__reachable_id__)'
        dist_leaf_sql = ''  if is_platform_pg() else 'DISTRIBUTED BY (__leaf_id__)'

        # core_points_table = unique_string(desp='core_points_table')
        core_points_table = 'core_points_table'
        plpy.execute("DROP TABLE IF EXISTS {0}".format(core_points_table))
        # core_edge_table = unique_string(desp='core_edge_table')
        core_edge_table = 'core_edge_table'
        plpy.execute("DROP TABLE IF EXISTS {0}".format(core_edge_table))

        if algorithm == KD_TREE:
            cur_source_table, border_table1, border_table2 = dbscan_kd(schema_madlib, source_table, id_column, expr_point, eps, min_samples, depth)

            kd_join_clause = "AND __t1__.__leaf_id__ = __t2__.__leaf_id__ "

            sql = """
                SELECT count(*), __leaf_id__ FROM {cur_source_table} GROUP BY __leaf_id__
                """.format(**locals())
            result = plpy.execute(sql)
            rt_counts_dict = {}
            for i in result:
                rt_counts_dict[i['__leaf_id__']] = int(i['count'])
            rt_counts_list = []
            for i in sorted(rt_counts_dict):
                rt_counts_list.append(rt_counts_dict[i])

            leaf_id_start = pow(2,depth)-1
            # rtree_step_table = unique_string(desp='rtree_step_table')
            rtree_step_table = 'rtree_step_table'
            t4 = datetime.now()
            plpy.execute("DROP TABLE IF EXISTS {0}".format(rtree_step_table))
            sql = """
            CREATE TABLE {rtree_step_table} AS
            SELECT __leaf_id__,
                   {schema_madlib}.rtree_step( {id_column},
                                               {expr_point},
                                               {eps},
                                               {min_samples},
                                               '{metric}',
                                               ARRAY{rt_counts_list},
                                               __leaf_id__
                                               )
            FROM {cur_source_table} GROUP BY __leaf_id__
            {dist_leaf_sql}
            """.format(**locals())
            plpy.execute(sql)

            t5 = datetime.now()
            plpy.info("rtree_step_table {0}".format(t5-t4))
            print_count(rtree_step_table)

            rt_edge_table = 'rt_edge_table'
            plpy.execute("DROP TABLE IF EXISTS {0}".format(rt_edge_table))
            sql = """
            CREATE TABLE {rt_edge_table} AS
            SELECT (unpacked_2d).src AS __src__, (unpacked_2d).dest AS __dest__
            FROM (
                SELECT {schema_madlib}.unpack_2d(rtree_step) AS unpacked_2d
                FROM {rtree_step_table}
                ) q1
            WHERE (unpacked_2d).src NOT IN (SELECT {id_column} FROM {border_table1})
            {dist_src_sql}
            """.format(**locals())
            plpy.execute(sql)
            t6 = datetime.now()
            plpy.info("rt core edge table {0}".format(t6-t5))
            print_count(core_edge_table)

            rt_core_points_table = 'rt_core_points_table'
            plpy.execute("DROP TABLE IF EXISTS {0}".format(rt_core_points_table))
            sql = """
                CREATE TABLE {rt_core_points_table} AS
                SELECT DISTINCT(__src__) AS {id_column} FROM {rt_edge_table}
                """.format(**locals())
            plpy.execute(sql)

            # # Start border
            border_distance_table = 'border_distance_table'
            plpy.execute("DROP TABLE IF EXISTS {0}".format(border_distance_table))
            sql = """
                CREATE TABLE {border_distance_table} AS
                SELECT __src__, __dest__ FROM (
                    SELECT  __t1__.{id_column} AS __src__,
                            __t2__.{id_column} AS __dest__,
                            {schema_madlib}.{metric}(
                                __t1__.{expr_point}, __t2__.{expr_point}) AS __dist__
                    FROM {border_table1} AS __t1__, {border_table2} AS __t2__)q1
                WHERE __dist__ < {eps}
                """.format(**locals())
            plpy.execute(sql)

            t6_1 = datetime.now()
            plpy.info("border core edge table {0}".format(t6_1-t6))

            border_core_points_table = 'border_core_points_table'
            plpy.execute("DROP TABLE IF EXISTS {0}".format(border_core_points_table))
            sql = """
                CREATE TABLE {border_core_points_table} AS
                SELECT * FROM (
                    SELECT __src__ AS {id_column}, count(*) AS __count__
                    FROM {border_distance_table} GROUP BY __src__) q1
                WHERE __count__ >= {min_samples}
                {dist_id_sql}
                """.format(**locals())
            plpy.execute(sql)

            sql = """
                CREATE TABLE {core_edge_table} AS
                SELECT __t1__.__src__, __t1__.__dest__
                FROM {rt_edge_table} __t1__ ,
                    (SELECT array_agg({id_column}) AS arr
                     FROM {rt_core_points_table}) __t2__
                WHERE  __t1__.__dest__ = ANY(arr)
                UNION
                SELECT __t1__.__src__, __t1__.__dest__
                FROM {border_distance_table} __t1__ ,
                    (SELECT array_agg({id_column}) AS arr
                     FROM {border_core_points_table}) __t2__
                WHERE  __t1__.__src__ = ANY(arr) AND __t1__.__dest__ = ANY(arr)
                """.format(**locals())
            plpy.execute(sql)

            sql = """
                CREATE TABLE {core_points_table} AS
                SELECT {id_column} FROM {border_core_points_table}
                UNION
                SELECT {id_column} FROM {rt_core_points_table}
                """.format(**locals())
            plpy.execute(sql)


            t7 = datetime.now()
            plpy.info("rt core points table {0}".format(t7-t6_1))
            print_count(core_points_table)

        else:
            cur_source_table = source_table
            kd_join_clause = ''

            t4 = datetime.now()

            # Calculate pairwise distances
            # distance_table = unique_string(desp='distance_table')
            distance_table = 'distance_table'
            plpy.execute("DROP TABLE IF EXISTS {0}".format(distance_table))

            sql = """
                CREATE TABLE {distance_table} AS
                SELECT __src__, __dest__ FROM (
                    SELECT  __t1__.{id_column} AS __src__,
                            __t2__.{id_column} AS __dest__,
                            {schema_madlib}.{metric}(
                                __t1__.{expr_point}, __t2__.{expr_point}) AS __dist__
                    FROM {cur_source_table} AS __t1__, {cur_source_table} AS __t2__)q1
                WHERE __dist__ < {eps}
                {dist_src_sql}
                """.format(**locals())
            plpy.execute(sql)

            t5 = datetime.now()
            plpy.info("dist table {0}".format(t5-t4))
            print_count(distance_table)
            # Find core points
            sql = """
                CREATE TABLE {core_points_table} AS
                SELECT * FROM (
                    SELECT __src__ AS {id_column}, count(*) AS __count__
                    FROM {distance_table} GROUP BY __src__) q1
                WHERE __count__ >= {min_samples}
                {dist_id_sql}
                """.format(**locals())
            plpy.execute(sql)

            t6 = datetime.now()
            plpy.info("core points table {0}".format(t6-t5))
            print_count(core_points_table)
            # Find the connections between core points to form the clusters
            sql = """
                CREATE TABLE {core_edge_table} AS
                SELECT __src__, __dest__
                FROM {distance_table} AS __t1__, (SELECT array_agg({id_column}) AS arr
                                                  FROM {core_points_table}) __t2__
                WHERE __t1__.__src__ = ANY(arr) AND __t1__.__dest__ = ANY(arr)
                {dist_src_sql}
            """.format(**locals())
            plpy.execute(sql)

            t7 = datetime.now()
            plpy.info("core edge {0}".format(t7-t6))
            print_count(core_edge_table)
        return
        # Start snowflake creation
        if is_platform_pg():
            sql = """
                SELECT count(*) FROM {core_edge_table}
                """.format(**locals())
            count = plpy.execute(sql)[0]['count']

            counts_list = [int(count)]
            # plpy.info(counts_list)
            seg_id = 0
            group_by_clause = ''
            dist_clause = ''

        else:
            sql = """
                SELECT count(*), gp_segment_id FROM {core_edge_table} GROUP BY gp_segment_id
                """.format(**locals())
            count_result = plpy.execute(sql)
            seg_num = get_seg_number()
            counts_list = [0]*seg_num
            for i in count_result:
                counts_list[int(i['gp_segment_id'])] = int(i['count'])
            seg_id = 'gp_segment_id'
            group_by_clause = 'GROUP BY gp_segment_id'
            dist_clause = 'DISTRIBUTED BY (seg_id)'

        dbscan_step_table = unique_string(desp='dbscan_step_table')
        dbscan_step_table = 'dbscan_step_table'
        plpy.execute("DROP TABLE IF EXISTS {0}".format(dbscan_step_table))
        sql = """
            CREATE TABLE {dbscan_step_table} AS
            SELECT {seg_id}::INTEGER AS seg_id,
                   {schema_madlib}.dbscan_step( __src__,
                                                __dest__,
                                                ARRAY{counts_list},
                                                {seg_id}
                                               )
            FROM {core_edge_table} {group_by_clause}
            {dist_clause}
        """.format(**locals())
        plpy.execute(sql)

        plpy.info("dbscan_step_table")
        print_count(dbscan_step_table)

        sf_edge_table = unique_string(desp='sf_edge_table')
        sf_edge_table = 'sf_edge_table'
        plpy.execute("DROP TABLE IF EXISTS {0}".format(sf_edge_table))

        sql = """
            CREATE TABLE {sf_edge_table} AS
            SELECT seg_id, (unpacked_2d).src AS __src__, (unpacked_2d).dest AS __dest__
            FROM (
                SELECT seg_id,
                       {schema_madlib}.unpack_2d(dbscan_step) AS unpacked_2d
                FROM {dbscan_step_table}
                ) q1
            {dist_clause}
            """.format(**locals())
        plpy.execute(sql)

        t7_5 = datetime.now()
        plpy.info("snowflake {0}".format(t7_5-t7))
        print_count(sf_edge_table)

        # Run wcc to get the min id for each cluster
        wcc(schema_madlib, core_points_table, id_column, sf_edge_table,
            'src=__src__, dest=__dest__', output_table, None)

        t8 = datetime.now()
        plpy.info("wcc {0}".format(t8-t7_5))

        plpy.execute("""
            ALTER TABLE {0}
            ADD COLUMN is_core_point BOOLEAN,
            ADD COLUMN __points__ DOUBLE PRECISION[]
            """.format(output_table))
        plpy.execute("""
            ALTER TABLE {0}
            RENAME COLUMN component_id TO cluster_id
            """.format(output_table))
        plpy.execute("""
            UPDATE {0}
            SET is_core_point = TRUE
        """.format(output_table))

        t9 = datetime.now()
        plpy.info("alter tables {0}".format(t9-t8))
        return

        # Find reachable points
        reachable_points_table = unique_string(desp='reachable_points_table')
        plpy.execute("DROP TABLE IF EXISTS {0}".format(reachable_points_table))
        sql = """
            CREATE TABLE {reachable_points_table} AS
                SELECT array_agg(__src__) AS __src_list__,
                       __dest__ AS __reachable_id__
                FROM {distance_table} AS __t1__,
                     (SELECT array_agg({id_column}) AS __arr__
                      FROM {core_points_table}) __t2__
                WHERE __src__ = ANY(__arr__) AND __dest__ != ALL(__arr__)
                GROUP BY __dest__
                {dist_reach_sql}
            """.format(**locals())
        plpy.execute(sql)

        t10 = datetime.now()
        plpy.info("reach table {0}".format(t10-t9))
        sql = """
            INSERT INTO {output_table}
            SELECT  __reachable_id__ as {id_column},
                    cluster_id,
                    FALSE AS is_core_point,
                    NULL AS __points__
            FROM {reachable_points_table} AS __t1__ INNER JOIN
                 {output_table} AS __t2__
                 ON (__src_list__[1] = {id_column})
            """.format(**locals())
        plpy.execute(sql)

        t11 = datetime.now()
        plpy.info("output table {0}".format(t11-t10))
        # Add features of points to the output table to use them for prediction
        sql = """
            UPDATE {output_table} AS __t1__
            SET __points__ = {expr_point}
            FROM {source_table} AS __t2__
            WHERE __t1__.{id_column} = __t2__.{id_column}
        """.format(**locals())
        plpy.execute(sql)

        t12 = datetime.now()
        plpy.info("output table up1 {0}".format(t12-t11))
        # Update the cluster ids to be consecutive
        sql = """
            UPDATE {output_table} AS __t1__
            SET cluster_id = new_id-1
            FROM (
                SELECT cluster_id, row_number() OVER(ORDER BY cluster_id) AS new_id
                FROM {output_table}
                GROUP BY cluster_id) __t2__
            WHERE __t1__.cluster_id = __t2__.cluster_id
        """.format(**locals())
        plpy.execute(sql)

        t13 = datetime.now()
        plpy.info("output table up2{0}".format(t13-t12))

        output_summary_table = add_postfix(output_table, '_summary')
        plpy.execute("DROP TABLE IF EXISTS {0}".format(output_summary_table))

        sql = """
            CREATE TABLE {output_summary_table} AS
            SELECT  '{id_column}'::VARCHAR AS id_column,
                    {eps}::DOUBLE PRECISION AS eps,
                    '{metric}'::VARCHAR AS metric
            """.format(**locals())
        plpy.execute(sql)

        # plpy.execute("DROP TABLE IF EXISTS {0}, {1}, {2}, {3}".format(
        #              distance_table, core_points_table, core_edge_table,
        #              reachable_points_table))

def dbscan_kd(schema_madlib, source_table, id_column, expr_point, eps, min_samples, depth):

    t1 = datetime.now()
    n_features = num_features(source_table, expr_point)

    kd_array, case_when_clause, border_cl1, border_cl2 = build_kd_tree(schema_madlib, source_table, expr_point, depth, n_features, eps)

    plpy.info("kd array: {0}".format(kd_array))
    kd_source_table = 'kd_source_table'
    kd_border_table1 = 'kd_border_table1'
    kd_border_table2 = 'kd_border_table2'

    t2 = datetime.now()
    plpy.info("build kd tree {0}".format(t2-t1))
    dist_leaf_sql = ''  if is_platform_pg() else 'DISTRIBUTED BY (__leaf_id__)'
    plpy.execute("DROP TABLE IF EXISTS {0}, {1}, {2}".format(kd_source_table, kd_border_table1, kd_border_table2))
    # sql = """
    #     CREATE TABLE {kd_source_table} AS
    #     SELECT {id_column}, {expr_point},
    #         unnest({schema_madlib}.get_leaf_list({expr_point},
    #                                       ARRAY{kd_array}::INTEGER[],
    #                                       {eps},
    #                                       {n_features})) AS __leaf_id__
    #     FROM {source_table}
    #     {dist_leaf_sql}
    # """.format(**locals())
    # plpy.execute(sql)
    output_sql = """
        CREATE TABLE {kd_source_table} AS
            SELECT *,
                   CASE {case_when_clause} END AS __leaf_id__
            FROM {source_table}
            {dist_leaf_sql}
        """.format(**locals())
    # plpy.info(output_sql)
    plpy.execute(output_sql)

    border_sql = """
        CREATE TABLE {kd_border_table1} AS
            SELECT *
            FROM {source_table}
            WHERE {border_cl1}
        """.format(**locals())
    # plpy.info(border_sql)
    plpy.execute(border_sql)

    border_sql = """
        CREATE TABLE {kd_border_table2} AS
            SELECT *
            FROM {source_table}
            WHERE {border_cl2}
        """.format(**locals())
    # plpy.info(border_sql)
    plpy.execute(border_sql)

    t3 = datetime.now()
    plpy.info("kd source table {0}".format(t3-t2))
    return kd_source_table, kd_border_table1, kd_border_table2



def build_kd_tree(schema_madlib, source_table, expr_point,
                  depth, n_features, eps, **kwargs):
    """
        KD-tree function to create a partitioning for KNN
        Args:
            @param schema_madlib        Name of the Madlib Schema
            @param source_table         Training data table
            @param output_table         Name of the table to store kd tree
            @param expr_point    Name of the column with training data
                                        or expression that evaluates to a
                                        numeric array
            @param depth                Depth of the kd tree
    """
    with MinWarning("error"):

        n_features = num_features(source_table, expr_point)

        clauses = [' 1=1 ']
        border_cl1 = ' 1!=1 '
        border_cl2 = ' 1!=1 '
        clause_counter = 0
        kd_array = []
        for curr_level in range(depth):
            curr_feature = (curr_level % n_features) + 1
            for curr_leaf in range(pow(2,curr_level)):
                clause = clauses[clause_counter]
                cutoff_sql = """
                    SELECT percentile_disc(0.5)
                           WITHIN GROUP (
                            ORDER BY ({expr_point})[{curr_feature}]
                           ) AS cutoff
                    FROM {source_table}
                    WHERE {clause}
                    """.format(**locals())

                cutoff = plpy.execute(cutoff_sql)[0]['cutoff']
                cutoff = "NULL" if cutoff is None else cutoff
                kd_array.append(cutoff)
                clause_counter += 1
                clauses.append(clause +
                               "AND ({expr_point})[{curr_feature}] < {cutoff} ".
                               format(**locals()))
                clauses.append(clause +
                               "AND ({expr_point})[{curr_feature}] >= {cutoff} ".
                               format(**locals()))
                border_cl1 = border_cl1 + """ OR (({expr_point})[{curr_feature}] >= {cutoff} - {eps}
                                            AND ({expr_point})[{curr_feature}] <= {cutoff} + {eps})
                                        """.format(**locals())
                border_cl2 = border_cl2 + """ OR (({expr_point})[{curr_feature}] >= {cutoff} - (2*{eps})
                                            AND ({expr_point})[{curr_feature}] <= {cutoff} + (2*{eps}))
                                        """.format(**locals())

        n_leaves = pow(2, depth)
        case_when_clause = '\n'.join(["WHEN {0} THEN {1}::INTEGER".format(cond, i)
                                     for i, cond in enumerate(clauses[-n_leaves:])])
        return kd_array, case_when_clause, border_cl1, border_cl2

# def get_leaf_list(schema_madlib, expr_point, kd_array, eps, n_features, **kwargs):

#     """
#     kd-tree is represented as an array or boundary values.
#     Array index -> tree node mapping:
#           0
#       +---^---+
#       1       2
#     +-^-+   +-^-+
#     3   4   5   6
#     In this case index i will have the following children: 2i+1 and 2i+2

#     """

#     node_q = [0]
#     section_list = []

#     while node_q:
#         current_node = node_q.pop()

#         if current_node > len(kd_array)-1:
#             section_list.append(current_node)
#         else:
#             curr_feature = int(floor(log(current_node+1,2))%n_features)
#             comp_value = expr_point[curr_feature]

#             if comp_value <= kd_array[current_node]+eps:
#                 node_q.append(2*current_node+1)

#             if comp_value >= kd_array[current_node]-eps:
#                 node_q.append(2*current_node+2)

#     return section_list



def dbscan_predict(schema_madlib, dbscan_table, new_point, **kwargs):

    with MinWarning("warning"):

        dbscan_summary_table = add_postfix(dbscan_table, '_summary')
        summary = plpy.execute("SELECT * FROM {0}".format(dbscan_summary_table))[0]

        eps = summary['eps']
        metric = summary['metric']
        sql = """
            SELECT cluster_id,
                   {schema_madlib}.{metric}(__points__, ARRAY{new_point}) as dist
            FROM {dbscan_table}
            WHERE is_core_point = TRUE
            ORDER BY dist LIMIT 1
            """.format(**locals())
        result = plpy.execute(sql)[0]
        dist = result['dist']
        if dist < eps:
            return result['cluster_id']
        else:
            return None

def pt(leaf_id, in_str):

    plpy.info("leaf {0}: {1}".format(leaf_id, in_str))

def rtree_transition(state, id_in, expr_points, eps, min_samples, metric, n_rows, leaf_id, **kwargs):

    SD = kwargs['SD']
    if not state:
        data = {}
        SD['counter{0}'.format(leaf_id)] = 0
    else:
        data = SD['data{0}'.format(leaf_id)]

    data[id_in] = expr_points
    SD['counter{0}'.format(leaf_id)] = SD['counter{0}'.format(leaf_id)]+1
    SD['data{0}'.format(leaf_id)] = data
    ret = [[-1,-1],[-1,-1]]

    # plpy.info("expr_points {0} for leaf {1}".format(expr_points, leaf_id))
    # plpy.info("SD['counter'] {0} for leaf {1}".format(SD['counter{0}'.format(leaf_id)], leaf_id))
    # plpy.info("SD['data'] {0} for leaf {1}".format(SD['data{0}'.format(leaf_id)], leaf_id))

    my_n_rows = n_rows[leaf_id]

    if SD['counter{0}'.format(leaf_id)] == my_n_rows:

        tt1 = datetime.now()
        # pt(leaf_id,"start rtree")

        core_counts = {}
        core_lists = {}
        p = index.Property()
        p.dimension = len(expr_points)
        idx = index.Index(properties=p)
        ret = []
        tt2 = datetime.now()
        # pt(leaf_id,"init rtree {0}".format(tt2-tt1))

        for key1, value1 in data.items():
            idx.add(key1,value1+value1,key1)

        tt3 = datetime.now()
        # pt(leaf_id,"fill rtree {0}".format(tt3-tt2))
        for key1, value1 in data.items():

            # tt4 = datetime.now()
            # pt(leaf_id,"start for key {0}".format(key1))
            v1 = []
            v2 = []
            for dim in value1:
                v1.append(dim-eps)
                v2.append(dim+eps)
            v = v1+v2
            hits = idx.intersection(v)

            # tt5 = datetime.now()
            # pt(leaf_id,"intr key {0} {1}".format(key1, tt5-tt4))
            if key1 not in core_counts:
                core_counts[key1] = 0
                core_lists[key1] = []

            for key2 in hits:
                # if key2 != key1:
                value2 = data[key2]
                distance = sqrt(sum([(a - b) ** 2 for a, b in zip(value1, value2)]))
                if distance <= eps:
                    core_counts[key1] += 1
                    core_lists[key1].append(key2)

            # tt6 = datetime.now()
            # pt(leaf_id,"hits key {0} {1}".format(key1, tt6-tt5))

        # plpy.info("core_counts {0} for leaf {1}".format(core_counts, leaf_id))
        # plpy.info("core_lists {0} for leaf {1}".format(core_lists, leaf_id))

        tt7 = datetime.now()
        # pt(leaf_id,"done rtree {0}".format(tt7-tt3))

        for key1, value1 in core_counts.items():
            if value1 >= min_samples:
                for key2 in core_lists[key1]:
                    ret.append([key1,key2])

        tt8 = datetime.now()
        # pt(leaf_id,"append {0}".format(tt8-tt7))

    return ret

def rtree_merge(state1, state2, **kwargs):

    if not state1:
        return state2
    elif not state2:
        return state1
    """
    core_dict1 = dict(zip(state1[0], state1[1]))
    core_dict2 = dict(zip(state2[0], state2[1]))

    for key1, value1 in core_dict1.items():

        if key1 not in core_dict2:
            core_dict2[key1] = value1
        else:
            core_dict2[key1] += value1

    ret = [core_dict2.keys()]
    ret.append(core_dict2.values())

    return ret
    """

def rtree_final(state, **kwargs):

    return state

"""
def dbscan_transition_old(state, id_in, expr_points, eps, min_samples, metric, n_rows, leaf_id, **kwargs):

    SD = kwargs['SD']
    if not state:
        data = {}
        SD['counter'] = 0
    else:
        data = SD['data']

    data[id_in] = expr_points
    SD['counter'] = SD['counter']+1
    ret = [[999],[999]]

    my_n_rows = n_rows[leaf_id]

    if SD['counter'] == my_n_rows:
        # plpy.info(data)

        cl_ids = {}
        clid_counter = 1
        cl_counts = {}
        eps_counts = {}
        for key1, value1 in data.items():
            for key2, value2 in data.items():
                # plpy.info("{0} {1}".format(key1, key2))
                if key1 < key2:
                    distance = sqrt(sum([(a - b) ** 2 for a, b in zip(value1, value2)]))
                    if distance <= eps:

                        cl_id_k1 = cl_ids[key1] if key1 in cl_ids else None
                        cl_id_k2 = cl_ids[key2] if key2 in cl_ids else None

                        eps_counts[key1] = 2 if key1 not in eps_counts else eps_counts[key1]+1
                        eps_counts[key2] = 2 if key2 not in eps_counts else eps_counts[key2]+1

                        if not cl_id_k1 and not cl_id_k2:
                            cl_ids[key1] = clid_counter
                            cl_ids[key2] = clid_counter
                            cl_counts[clid_counter] = 2
                            clid_counter += 1
                        elif cl_id_k1 and not cl_id_k2:
                            cl_ids[key2] = cl_id_k1
                            cl_counts[cl_ids[key1]] += 1
                        elif not cl_id_k1 and cl_id_k2:
                            cl_ids[key1] = cl_id_k2
                            cl_counts[cl_ids[key2]] += 1
                        else:
                            if cl_id_k1 != cl_id_k2:
                                if cl_counts[cl_id_k1] > cl_counts[cl_id_k2]:
                                    for chkey,chvalue in cl_ids.items():
                                        if chvalue == cl_id_k2:
                                            cl_ids[chkey] = cl_id_k1
                                    cl_counts[cl_id_k1] += cl_counts[cl_id_k2]
                                    cl_counts[cl_id_k2] = 0
                                else:
                                    for chkey,chvalue in cl_ids.items():
                                        if chvalue == cl_id_k1:
                                            cl_ids[chkey] = cl_id_k2
                                    cl_counts[cl_id_k2] += cl_counts[cl_id_k1]
                                    cl_counts[cl_id_k1] = 0

                # plpy.info("cl ids {0}".format(cl_ids))
                # plpy.info("eps_counts {0}".format(eps_counts))

        # plpy.info(eps_counts)
        if eps_counts:
            for key1, value1 in eps_counts.items():
                if value1 < min_samples:
                    cl_ids.pop(key1)

            # plpy.info("cl ids {0}".format(cl_ids))
            ret = [cl_ids.keys()]
            ret.append(cl_ids.values())
            # plpy.info(ret)

    SD['data'] = data

    return ret
"""

def dbscan_transition(state, src, dest, n_rows, gp_segment_id, **kwargs):

    SD = kwargs['SD']
    if not state:
        data = []
        SD['counter'] = 0
    else:
        data = SD['data']

    counter = SD['counter']

    data.append([src,dest])
    ret = [[-1,-1],[-1,-1]]

    my_n_rows = n_rows[gp_segment_id]

    if len(data) == my_n_rows:
        # plpy.info(data)

        cl_ids = {}
        clid_counter = 1
        cl_counts = {}
        for i in data:

            key1 = i[0]
            key2 = i[1]

            cl_id_k1 = cl_ids[key1] if key1 in cl_ids else None
            cl_id_k2 = cl_ids[key2] if key2 in cl_ids else None

            if not cl_id_k1 and not cl_id_k2:
                cl_ids[key1] = clid_counter
                cl_ids[key2] = clid_counter
                cl_counts[clid_counter] = 2
                clid_counter += 1
            elif cl_id_k1 and not cl_id_k2:
                cl_ids[key2] = cl_id_k1
                cl_counts[cl_ids[key1]] += 1
            elif not cl_id_k1 and cl_id_k2:
                cl_ids[key1] = cl_id_k2
                cl_counts[cl_ids[key2]] += 1
            else:
                if cl_id_k1 != cl_id_k2:
                    if cl_counts[cl_id_k1] > cl_counts[cl_id_k2]:
                        for chkey,chvalue in cl_ids.items():
                            if chvalue == cl_id_k2:
                                cl_ids[chkey] = cl_id_k1
                        cl_counts[cl_id_k1] += cl_counts[cl_id_k2]
                        cl_counts[cl_id_k2] = 0
                    else:
                        for chkey,chvalue in cl_ids.items():
                            if chvalue == cl_id_k1:
                                cl_ids[chkey] = cl_id_k2
                        cl_counts[cl_id_k2] += cl_counts[cl_id_k1]
                        cl_counts[cl_id_k1] = 0

                # plpy.info("cl ids {0}".format(cl_ids))
                # plpy.info("cl_counts {0}".format(cl_counts))

        # plpy.info(eps_counts)
        if cl_ids:

            running_cl_id = -1
            running_sf_center = -1
            ret = []
            for vertex_id, vertex_cl in sorted(cl_ids.items(), key=lambda item: item[1]):

                # Check if we are still in the same snowflake
                if vertex_cl != running_cl_id:

                    running_sf_center = vertex_id
                    running_cl_id = vertex_cl
                else:
                    ret.append([running_sf_center,vertex_id])
            # plpy.info("cl ids {0}".format(cl_ids))
            # plpy.info(ret)

    SD['data'] = data

    return ret


def dbscan_merge(state1, state2, **kwargs):

    if state1:
        return state1
    else:
        return state2

def dbscan_final(state, **kwargs):

    return state

def _validate_dbscan(schema_madlib, source_table, output_table, id_column, expr_point, eps, min_samples, metric, algorithm, depth):

    input_tbl_valid(source_table, 'dbscan')
    output_tbl_valid(output_table, 'dbscan')
    output_summary_table = add_postfix(output_table, '_summary')
    output_tbl_valid(output_summary_table, 'dbscan')

    cols_in_tbl_valid(source_table, [id_column], 'dbscan')

    _assert(is_var_valid(source_table, expr_point),
            "dbscan error: {0} is an invalid column name or "
            "expression for expr_point param".format(expr_point))

    point_col_type = get_expr_type(expr_point, source_table)
    _assert(is_valid_psql_type(point_col_type, NUMERIC | ONLY_ARRAY),
            "dbscan Error: Feature column or expression '{0}' in train table is not"
            " a numeric array.".format(expr_point))

    _assert(eps > 0, "dbscan Error: eps has to be a positive number")

    _assert(min_samples > 0, "dbscan Error: min_samples has to be a positive number")

    fn_dist_list = ['dist_norm1', 'dist_norm2', 'squared_dist_norm2', 'dist_angle', 'dist_tanimoto']
    _assert(metric in fn_dist_list, "dbscan Error: metric has to be one of the madlib defined distance functions")

    _assert(depth > 0, "dbscan Error: depth has to be a positive number")


def dbscan_help(schema_madlib, message=None, **kwargs):
    """
    Help function for dbscan

    Args:
        @param schema_madlib
        @param message: string, Help message string
        @param kwargs

    Returns:
        String. Help/usage information
    """
    if message is not None and \
            message.lower() in ("usage", "help", "?"):
        help_string = """
-----------------------------------------------------------------------
                            USAGE
-----------------------------------------------------------------------
SELECT {schema_madlib}.dbscan(
    source_table,       -- Name of the training data table
    output_table,       -- Name of the output table
    id_column,          -- Name of id column in source_table
    expr_point,         -- Column name or expression for data points
    eps,                -- The minimum radius of a cluster
    min_samples,        -- The minimum size of a cluster
    metric,             -- The name of the function to use to calculate the
                        -- distance
    algorithm           -- The algorithm to use for dbscan.
    );

-----------------------------------------------------------------------
                            OUTPUT
-----------------------------------------------------------------------
The output of the dbscan function is a table with the following columns:

id_column           The ids of test data point
cluster_id          The id of the points associated cluster
is_core_point       Boolean column that indicates if the point is core or not
points              The column or expression for the data point
"""
    else:
        help_string = """
----------------------------------------------------------------------------
                                SUMMARY
----------------------------------------------------------------------------
DBSCAN is a density-based clustering algorithm. Given a set of points in
some space, it groups together points that are closely packed together
(points with many nearby neighbors), marking as outliers points that lie
alone in low-density regions (whose nearest neighbors are too far away).
--
For an overview on usage, run:
SELECT {schema_madlib}.dbscan('usage');
SELECT {schema_madlib}.dbscan_predict('usage');
"""
    return help_string.format(schema_madlib=schema_madlib)
# ------------------------------------------------------------------------------

def dbscan_predict_help(schema_madlib, message=None, **kwargs):
    """
    Help function for dbscan

    Args:
        @param schema_madlib
        @param message: string, Help message string
        @param kwargs

    Returns:
        String. Help/usage information
    """
    if message is not None and \
            message.lower() in ("usage", "help", "?"):
        help_string = """
-----------------------------------------------------------------------
                            USAGE
-----------------------------------------------------------------------
SELECT {schema_madlib}.dbscan_predict(
    dbscan_table,       -- Name of the tdbscan output table
    new_point           -- Double precision array representing the point
                        -- for prediction
    );

-----------------------------------------------------------------------
                            OUTPUT
-----------------------------------------------------------------------
The output of the dbscan_predict is an integer indicating the cluster_id
of given point
"""
    else:
        help_string = """
----------------------------------------------------------------------------
                                SUMMARY
----------------------------------------------------------------------------
DBSCAN is a density-based clustering algorithm. Given a set of points in
some space, it groups together points that are closely packed together
(points with many nearby neighbors), marking as outliers points that lie
alone in low-density regions (whose nearest neighbors are too far away).
--
For an overview on usage, run:
SELECT {schema_madlib}.dbscan('usage');
SELECT {schema_madlib}.dbscan_predict('usage');
"""
    return help_string.format(schema_madlib=schema_madlib)
# ------------------------------------------------------------------------------

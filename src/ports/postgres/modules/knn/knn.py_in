# coding=utf-8
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

"""
@file knn.py_in

@brief knn: K-Nearest Neighbors for regression and classification

@namespace knn

"""

import plpy
import copy
from collections import defaultdict
from math import floor
from math import log
from math import ceil
from utilities.control import MinWarning
from utilities.utilities import _assert
from utilities.utilities import add_postfix
from utilities.utilities import py_list_to_sql_string, is_valid_psql_type
from utilities.utilities import unique_string
from utilities.utilities import NUMERIC, ONLY_ARRAY
from utilities.utilities import is_valid_psql_type
from utilities.utilities import is_pg_major_version_less_than
from utilities.utilities import num_features
from utilities.validate_args import array_col_has_no_null
from utilities.validate_args import cols_in_tbl_valid
from utilities.validate_args import drop_tables
from utilities.validate_args import get_cols
from utilities.validate_args import get_expr_type
from utilities.validate_args import input_tbl_valid, output_tbl_valid
from utilities.validate_args import is_col_array
from utilities.validate_args import is_var_valid
from utilities.validate_args import quote_ident

MAX_WEIGHT_ZERO_DIST = 1e6
EPSILON = 1/MAX_WEIGHT_ZERO_DIST


def knn_validate_src(schema_madlib, point_source, point_column_name, point_id,
                     label_column_name, test_source, test_column_name,
                     test_id, output_table, k, output_neighbors, fn_dist,
                     **kwargs):
    input_tbl_valid(point_source, 'kNN')
    input_tbl_valid(test_source, 'kNN')
    output_tbl_valid(output_table, 'kNN')

    _assert(label_column_name or output_neighbors,
            "kNN error: Either label_column_name or "
            "output_neighbors has to be inputed.")

    if label_column_name and label_column_name.strip():
        cols_in_tbl_valid(point_source, [label_column_name], 'kNN')

    _assert(is_var_valid(point_source, point_column_name),
            "kNN error: {0} is an invalid column name or "
            "expression for point_column_name param".format(point_column_name))
    point_col_type = get_expr_type(point_column_name, point_source)
    _assert(is_valid_psql_type(point_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in train table is not"
            " an array.".format(point_column_name))

    _assert(is_var_valid(test_source, test_column_name),
            "kNN error: {0} is an invalid column name or expression for "
            "test_column_name param".format(test_column_name))
    test_col_type = get_expr_type(test_column_name, test_source)
    _assert(is_valid_psql_type(test_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in test table is not"
            " an array.".format(test_column_name))

    cols_in_tbl_valid(point_source, [point_id], 'kNN')
    cols_in_tbl_valid(test_source, [test_id], 'kNN')

    if not array_col_has_no_null(point_source, point_column_name):
        plpy.error("kNN Error: Feature column '{0}' in train table has some"
                   " NULL values.".format(point_column_name))
    if not array_col_has_no_null(test_source, test_column_name):
        plpy.error("kNN Error: Feature column '{0}' in test table has some"
                   " NULL values.".format(test_column_name))

    if k <= 0:
        plpy.error("kNN Error: k={0} is an invalid value, must be greater "
                   "than 0.".format(k))

    bound = plpy.execute("SELECT {k} <= count(*) AS bound FROM {tbl}".
                         format(k=k, tbl=point_source))[0]['bound']
    if not bound:
        plpy.error("kNN Error: k={0} is greater than number of rows in"
                   " training table.".format(k))

    if label_column_name:
        col_type = get_expr_type(label_column_name, point_source).lower()
        if col_type not in ['integer', 'double precision', 'float', 'boolean']:
            plpy.error("kNN error: Invalid data type '{0}' for"
                       " label_column_name in table '{1}'.".
                       format(col_type, point_source))

    col_type_test = get_expr_type(test_id, test_source).lower()
    if col_type_test not in ['integer']:
        plpy.error("kNN Error: Invalid data type '{0}' for"
                   " test_id column in table '{1}'.".
                   format(col_type_test, test_source))

    if fn_dist:
        fn_dist = fn_dist.lower().strip()
        dist_functions = set(["{0}.{1}".format(schema_madlib, dist) for dist in
                              ('dist_norm1', 'dist_norm2',
                               'squared_dist_norm2', 'dist_angle',
                               'dist_tanimoto')])

        profunc = ("proisagg = TRUE" if is_pg_major_version_less_than(schema_madlib, 11)
              else "prokind = 'a'")

        is_invalid_func = plpy.execute("""
            SELECT prorettype != 'DOUBLE PRECISION'::regtype OR
                   {profunc} AS OUTPUT
            FROM pg_proc
            WHERE oid='{fn_dist}(DOUBLE PRECISION[], DOUBLE PRECISION[])'::regprocedure;
            """.format(fn_dist=fn_dist, profunc=profunc))[0]['output']

        if is_invalid_func or (fn_dist not in dist_functions):
            plpy.error("KNN error: Distance function ({0}) has invalid signature "
                       "or is not a simple function.".format(fn_dist))

    return k
# ------------------------------------------------------------------------------


def kd_tree(schema_madlib, source_table, output_table, point_column_name, depth,
            r_id, dim, **kwargs):
    """
        KD-tree function to create a partitioning for KNN
        Args:
            @param schema_madlib        Name of the Madlib Schema
            @param source_table         Training data table
            @param output_table         Name of the table to store kd tree
            @param point_column_name    Name of the column with training data
                                        or expression that evaluates to a
                                        numeric array
            @param depth                Depth of the kd tree
            @param r_id                 Name of the region id column
            @param dim                  Name of the dimention column
            #TODO add r_border everywhere

    """
    with MinWarning("error"):

        validate_kd_tree(source_table, output_table, point_column_name, depth)
        n_features = num_features(source_table, point_column_name)

        clauses = [" WHERE 1=1 "]
        cutoffs = []
        regions = [[("'-Infinity'","'Infinity'") for i in range(n_features)]]
        regions_table = add_postfix(output_table, "_regions")
        clause_counter = 0
        current_feature = 1
        for curr_level in range(depth):
            for curr_leaf in range(pow(2,curr_level)):
                clause = clauses[clause_counter]
                cutoff_sql = """
                    SELECT percentile_disc(0.5)
                           WITHIN GROUP (
                            ORDER BY {point_column_name}[{current_feature}]
                           ) AS cutoff
                    FROM {source_table}
                    {clause}
                    """.format(**locals())

                cutoff = plpy.execute(cutoff_sql)[0]['cutoff']

                region = regions.pop(0)
                region1 = copy.deepcopy(region)
                region2 = copy.deepcopy(region)
                region1[current_feature-1] = (region[current_feature-1][0], cutoff)
                region2[current_feature-1] = (cutoff, region[current_feature-1][1])
                regions.append(region1)
                regions.append(region2)

                cutoffs.append(cutoff if cutoff is not None else "NULL")
                clause_counter += 1
                clauses.append(clause +
                               "AND {point_column_name}[{current_feature}]"
                               " < {cutoff} ".format(**locals()))
                clauses.append(clause +
                               "AND {point_column_name}[{current_feature}]"
                               " >= {cutoff} ".format(**locals()))
            current_feature = current_feature % n_features + 1

        output_table_tree = add_postfix(output_table, "_tree")
        plpy.execute("CREATE TABLE {0} AS "
                     "SELECT ('{{ {1} }}')::DOUBLE PRECISION[] AS tree".
                     format(output_table_tree,
                            " ,".join(map(str, cutoffs))))

        plpy.execute("""
            CREATE TABLE {regions_table} (
                {r_id} INTEGER, {dim} INTEGER, {r_border} DOUBLE PRECISION)
            """.format(**locals()))

        for i, region in enumerate(regions,0):
            for j, border in enumerate(region,0):
                for k in border:
                    plpy.execute("""
                        INSERT INTO {regions_table}
                            ({r_id}, {dim}, {r_border})
                        VALUES ({i}, {j}+1, {k})""".format(**locals())
                        )

        n_leaves = pow(2,depth)
        case_when_clause = ["WHEN {0} THEN {1}::INTEGER".format(cond[14:], i)
                                     for i, cond in enumerate(clauses[-n_leaves:])]
        output_sql = """
            CREATE TABLE {output_table} AS
                SELECT *, CASE {cases} END AS {r_id}
                FROM {source_table}""".format(
                    cases = ' '.join(case_when_clause),**locals())
        plpy.execute(output_sql)
# ------------------------------------------------------------------------------

def validate_kd_tree(source_table, output_table, point_column_name, depth):

    input_tbl_valid(source_table, 'kd_tree')
    output_tbl_valid(output_table, 'kd_tree')
    output_tbl_valid(output_table+"_tree", 'kd_tree')

    _assert(is_var_valid(source_table, point_column_name),
            "kd_tree error: {0} is an invalid column name or expression for "
            "point_column_name param".format(point_column_name))
    point_col_type = get_expr_type(point_column_name, source_table)
    _assert(is_valid_psql_type(point_col_type, NUMERIC | ONLY_ARRAY),
            "kNN Error: Feature column or expression '{0}' in train table is not"
            " an array.".format(point_column_name))
    if depth <= 0:
        plpy.error("kNN Error: depth={0} is an invalid value, must be greater "
                   "than 0.".format(depth))
# ------------------------------------------------------------------------------

def knn_tree(schema_madlib, kd_out, point_source, point_column_name, point_id,
             label_column_name, test_source, test_column_name, test_id,
             interim_table, in_k, output_neighbors, fn_dist, weighted_avg,
             quant, r_id, dim, label_out, comma_label_out_alias, label_name,
             train, train_id, dist_inverse, **kwargs):
    """
        KNN function to find the K Nearest neighbours
        Args:
            @param schema_madlib        Name of the Madlib Schema
            @param kd_out               Name of the kd tree table
            @param point_source         Training data table
            @param point_column_name    Name of the column with training data
                                        or expression that evaluates to a
                                        numeric array
            @param point_id             Name of the column having ids of data
                                        point in train data table
                                        points.
            @param label_column_name    Name of the column with labels/values
                                        of training data points.
            @param test_source          Name of the table containing the test
                                        data points.
            @param test_column_name     Name of the column with testing data
                                        points or expression that evaluates to a
                                        numeric array
            @param test_id              Name of the column having ids of data
                                        points in test data table.
            @param interim_table        Name of the table to store interim
                                        results.
            @param in_k                 default: 1. Number of nearest
                                        neighbors to consider
            @param output_neighbours    Outputs the list of k-nearest neighbors
                                        that were used in the voting/averaging.
            @param fn_dist              Distance metrics function. Default is
                                        squared_dist_norm2. Following functions
                                        are supported :
                                        dist_norm1 , dist_norm2,squared_dist_norm2,
                                        dist_angle , dist_tanimoto
                                        Or user defined function with signature
                                        DOUBLE PRECISION[] x, DOUBLE PRECISION[] y -> DOUBLE PRECISION
            @param weighted_avg         Calculates the Regression or classication of k-NN using
                                        the weighted average method.
            @param quant
            @param r_id                 Name of the region id column
            @param dim                  Name of the dimention column
            Following parameters are passed to ensure the interim table has
            identical features to non-kd-tree implementation
            @param label_out
            @param comma_label_out_alias
            @param label_name
            @param train
            @param train_id
            @param dist_inverse
    """
    with MinWarning("error"):

        tree_model = add_postfix(kd_out, "_tree")
        regions_table = add_postfix(kd_out, "_regions")
        n_features = num_features(test_source, point_column_name)

        tree = plpy.execute("SELECT * FROM {0}".format(tree_model))[0]['tree']
        # 'tree' contains only non-leaf nodes,
        # hence 'n_leaves' is always 1 more than len(tree)
        n_leaves = len(tree)+1

        depth = int(log(n_leaves, 2))

        clause_counter = 0
        tree_counter = 0
        current_feature = 1
        clauses = [" 1 = 1 " for i in range(n_leaves)]

        for i in range(depth):
            repeat_num = n_leaves / (pow(2, i + 1))

            for j in range(0, pow(2, i)):
                cutoff = tree[tree_counter]
                for k in range(repeat_num):
                    clause = clauses.pop(0)
                    clause += (" AND {point_column_name}[{current_feature}] "
                                "< {cutoff} ".format(**locals()))
                    clauses.append(clause)
                for k in range(repeat_num):
                    clause = clauses.pop(0)
                    clause += (" AND {point_column_name}[{current_feature}] "
                                ">= {cutoff} ".format(**locals()))
                    clauses.append(clause)
                tree_counter += 1

            current_feature = current_feature % n_features + 1


        test_view = unique_string("test_view")
        plpy.execute("DROP VIEW IF EXISTS {test_view}".format(**locals()))
        test_view_case_when = '\n'.join(["WHEN {0} THEN {1}::INTEGER".
                                   format(clauses[i], i) for i in range(n_leaves)])
        test_view_sql = """
            CREATE VIEW {test_view} AS
                SELECT {point_id},
                       {point_column_name}::DOUBLE PRECISION[],
                       CASE
                        {test_view_case_when}
                       END AS {r_id}
                FROM {test_source}""".format(**locals())
        plpy.execute(test_view_sql.format())

        joined_regions = unique_string("joined_regions")
        plpy.execute("DROP TABLE IF EXISTS {joined_regions}".format(**locals()))
        plpy.execute("""
            CREATE TABLE {joined_regions} AS
                SELECT *
                FROM {test_view} INNER JOIN {regions_table}
                    USING ({r_id})
            """.format(**locals()))


        exp_table = unique_string("exp_table")
        old_data = unique_string("exp_table")

        plpy.execute("""
            CREATE TABLE {exp_table} AS
            SELECT {point_id}, {point_column_name}::DOUBLE PRECISION[],
                   {point_column_name}::DOUBLE PRECISION[] AS {old_data}, {dim},
                   CASE
                        WHEN {r_border}-{point_column_name}[{dim}] > 0
                        THEN {r_border}+{EPSILON}
                        ELSE {r_border}-{EPSILON} END AS border_eps
            FROM {joined_regions} INNER JOIN (
                    SELECT {point_id},
                           percentile_disc({quant}) WITHIN GROUP
                            (ORDER BY
                                abs({r_border}-{point_column_name}[{dim}])
                            ) min_border
                    FROM {joined_regions} GROUP BY {point_id}
                )q1 USING ({point_id})
            WHERE abs({r_border}-{point_column_name}[{dim}]) <= min_border
            """.format(EPSILON=EPSILON, **locals()))

        plpy.execute("""
            UPDATE {exp_table} SET {point_column_name}[{dim}]=border_eps
            """.format(**locals()))

        ext_test_view = unique_string("ext_test_view")
        plpy.execute("DROP VIEW IF EXISTS {ext_test_view}".format(**locals()))
        ext_test_view_sql = """ CREATE VIEW {ext_test_view} AS
            SELECT * FROM {test_view}
            UNION
            SELECT {point_id}, {old_data} AS {point_column_name},
                CASE {test_view_case_when} END AS {r_id}
            FROM {exp_table}
            """.format(**locals())
        plpy.execute(ext_test_view_sql)

        raise ValueError('Debug')
        sql = """ CREATE TABLE {interim_table} AS
            SELECT * FROM (
                SELECT row_number() OVER (PARTITION BY {test_id}
                                          ORDER BY dist) AS r,
                       {test_id}, {train_id}, dist,
                       CASE WHEN dist = 0.0 THEN {max_weight_zero_dist}
                                 ELSE 1.0 / dist
                            END AS {dist_inverse}
                            {comma_label_out_alias}
                FROM (
                    SELECT {train}.{r_id} AS tr_{r_id}, test.{r_id} AS test_{r_id},
                           {train}.{point_id} AS {train_id},
                           test.{test_id} AS {test_id},
                           {fn_dist}({train}.data,test.data) AS dist
                           {label_out}
                    FROM {kd_out} AS {train} INNER JOIN {ext_test_view} AS test
                        ON {train}.{r_id} = test.{r_id}
                    ) q1
                )q2
            WHERE r <= {in_k}""".format(max_weight_zero_dist=MAX_WEIGHT_ZERO_DIST,
                **locals())
        plpy.execute(sql)

        plpy.execute("DROP VIEW IF EXISTS {test_view} CASCADE".format(**locals()))
        drop_tables([joined_regions, exp_table])

        raise ValueError('Debug')
        return
# ------------------------------------------------------------------------------


def knn(schema_madlib, point_source, point_column_name, point_id,
        label_column_name, test_source, test_column_name, test_id, output_table,
        k, output_neighbors, fn_dist, weighted_avg, use_kdtree, depth, quant, **kwargs):
    """
        KNN function to find the K Nearest neighbours
        Args:
            @param schema_madlib        Name of the Madlib Schema
            @param point_source         Training data table
            @param point_column_name    Name of the column with training data
                                        or expression that evaluates to a
                                        numeric array
            @param point_id             Name of the column having ids of data
                                        point in train data table
                                        points.
            @param label_column_name    Name of the column with labels/values
                                        of training data points.
            @param test_source          Name of the table containing the test
                                        data points.
            @param test_column_name     Name of the column with testing data
                                        points or expression that evaluates to a
                                        numeric array
            @param test_id              Name of the column having ids of data
                                        points in test data table.
            @param output_table         Name of the table to store final
                                        results.
            @param k                    default: 1. Number of nearest
                                        neighbors to consider
            @param output_neighbours    Outputs the list of k-nearest neighbors
                                        that were used in the voting/averaging.
            @param fn_dist              Distance metrics function. Default is
                                        squared_dist_norm2. Following functions
                                        are supported :
                                        dist_norm1 , dist_norm2,squared_dist_norm2,
                                        dist_angle , dist_tanimoto
                                        Or user defined function with signature
                                        DOUBLE PRECISION[] x, DOUBLE PRECISION[] y -> DOUBLE PRECISION
            @param weighted_avg         Calculates the Regression or classication of k-NN using
                                        the weighted average method.
    """
    with MinWarning('warning'):
        output_neighbors = True if output_neighbors is None else output_neighbors
        if k is None:
            k = 1
        knn_validate_src(schema_madlib, point_source,
                         point_column_name, point_id, label_column_name,
                         test_source, test_column_name, test_id,
                         output_table, k, output_neighbors, fn_dist)

        # Unique Strings
        x_temp_table = unique_string(desp='x_temp_table')
        y_temp_table = unique_string(desp='y_temp_table')
        label_col_temp = unique_string(desp='label_col_temp')
        test_id_temp = unique_string(desp='test_id_temp')
        train = unique_string(desp='train')
        test = unique_string(desp='test')
        p_col_name = unique_string(desp='p_col_name')
        t_col_name = unique_string(desp='t_col_name')
        dist = unique_string(desp='dist')
        train_id = unique_string(desp='train_id')
        dist_inverse = unique_string(desp='dist_inverse')
        r = unique_string(desp='r')
        r_id = unique_string(desp='r_id')
        dim = unique_string(desp='dim')

        if not fn_dist:
            fn_dist = '{0}.squared_dist_norm2'.format(schema_madlib)

        fn_dist = fn_dist.lower().strip()
        interim_table = unique_string(desp='interim_table')

        pred_out = ""
        knn_neighbors = ""
        label_out = ""
        cast_to_int = ""
        view_def = ""
        view_join = ""
        view_grp_by = ""

        if label_column_name:
            label_column_type = get_expr_type(
                label_column_name, point_source).lower()
            if label_column_type in ['boolean', 'integer', 'text']:
                is_classification = True
                cast_to_int = '::INTEGER'
            else:
                is_classification = False

            if is_classification:
                if weighted_avg:
                    # This view is to calculate the max value of sum of the 1/distance grouped by label and Id.
                    # And this max value will be the prediction for the
                    # classification model.
                    view_def = """
                        WITH vw AS (
                            SELECT DISTINCT ON({test_id_temp})
                                {test_id_temp},
                                last_value(data_sum) OVER (
                                    PARTITION BY {test_id_temp}
                                    ORDER BY data_sum, {label_col_temp}
                                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
                                    ) AS data_dist ,
                                last_value({label_col_temp}) OVER (
                                    PARTITION BY {test_id_temp}
                                    ORDER BY data_sum, {label_col_temp}
                                    ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
                                    ) AS {label_col_temp}
                            FROM   (
                                SELECT
                                    {test_id_temp},
                                    {label_col_temp},
                                    sum({dist_inverse}) data_sum
                                FROM {interim_table}
                                GROUP BY {test_id_temp},
                                         {label_col_temp}
                            ) a
                        )
                        """.format(**locals())
                    # This join is needed to get the max value of predicion
                    # calculated above
                    view_join = (" JOIN vw ON knn_temp.{0} = vw.{0}".
                                 format(test_id_temp))
                    view_grp_by = ", vw.{0}".format(label_col_temp)
                    pred_out = ", vw.{0}".format(label_col_temp)
                else:
                    pred_out = ", {0}.mode({1})".format(
                        schema_madlib, label_col_temp)
            else:
                if weighted_avg:
                    pred_out = (", sum({0} * {dist_inverse}) / sum({dist_inverse})".
                                format(label_col_temp, dist_inverse=dist_inverse))
                else:
                    pred_out = ", avg({0})".format(label_col_temp)

            pred_out += " AS prediction"
            label_out = (", {train}.{label_column_name}{cast_to_int}"
                         " AS {label_col_temp}").format(**locals())
            comma_label_out_alias = ', ' + label_col_temp
            label_name = ", {label_column_name}".format(
                label_column_name=label_column_name)

        else:
            pred_out = ""
            label_out = ""
            comma_label_out_alias = ""
            label_name = ""

        # interim_table picks the 'k' nearest neighbors for each test point
        if output_neighbors:
            knn_neighbors = (", array_agg(knn_temp.{train_id} ORDER BY "
                             "knn_temp.{dist_inverse} DESC) AS k_nearest_neighbours ").format(**locals())
        else:
            knn_neighbors = ''

        if use_kdtree:
            #CALL new funcs

            size = plpy.execute("SELECT count(*) AS count FROM {0}".format(point_source))[0]['count']
            if depth < 1:
                use_kdtree = False
            else:
                kd_output_table = unique_string(desp='kd_tree')
                kd_tree(schema_madlib, point_source, kd_output_table,
                        point_column_name, depth, r_id, dim)
                knn_tree(schema_madlib, kd_output_table, point_source,
                         point_column_name, point_id, label_column_name,
                         test_source, test_column_name, test_id, interim_table,
                         k, output_neighbors, fn_dist, weighted_avg, quant,
                         r_id, dim, label_out, comma_label_out_alias, label_name,
                         train, train_id, dist_inverse)
                test_id_temp = test_id
        if not use_kdtree:
            plpy.execute("""
                CREATE TABLE {interim_table} AS
                    SELECT * FROM (
                        SELECT row_number() over
                                (partition by {test_id_temp} order by {dist}) AS {r},
                                {test_id_temp},
                                {train_id},
                                CASE WHEN {dist} = 0.0 THEN {max_weight_zero_dist}
                                     ELSE 1.0 / {dist}
                                END AS {dist_inverse}
                                {comma_label_out_alias}
                        FROM (
                            SELECT {test}.{test_id} AS {test_id_temp},
                                {train}.{point_id} as {train_id},
                                {fn_dist}(
                                    {p_col_name},
                                    {t_col_name})
                                AS {dist}
                                {label_out}
                                FROM
                                (
                                SELECT {point_id} , {point_column_name} as {p_col_name} {label_name} from {point_source}
                                ) {train},
                                (
                                SELECT {test_id} ,{test_column_name} as {t_col_name} from {test_source}
                                ) {test}
                            ) {x_temp_table}
                        ) {y_temp_table}
                WHERE {y_temp_table}.{r} <= {k}
                """.format(max_weight_zero_dist=MAX_WEIGHT_ZERO_DIST, **locals()))

        sql = """
            CREATE TABLE {output_table} AS
                {view_def}
                SELECT
                    knn_temp.{test_id_temp} AS id,
                    {test_column_name} as "{test_column_name}"
                    {pred_out}
                    {knn_neighbors}
                FROM
                    {interim_table}  AS knn_temp
                    JOIN
                    {test_source} AS knn_test
                ON knn_temp.{test_id_temp} = knn_test.{test_id}
                    {view_join}
                GROUP BY knn_temp.{test_id_temp},
                    {test_column_name}
                         {view_grp_by}
            """
        plpy.execute(sql.format(**locals()))
        plpy.execute("DROP TABLE IF EXISTS {0}".format(interim_table))
        return
# ------------------------------------------------------------------------------


def knn_help(schema_madlib, message, **kwargs):
    """
    Help function for knn

    Args:
        @param schema_madlib
        @param message: string, Help message string
        @param kwargs

    Returns:
        String. Help/usage information
    """
    if message is not None and \
            message.lower() in ("usage", "help", "?"):
        help_string = """
-----------------------------------------------------------------------
                            USAGE
-----------------------------------------------------------------------
SELECT {schema_madlib}.knn(
    point_source,       -- Training data table having training features as vector column and labels
    point_column_name,  -- Name of column having feature vectors in training data table
    point_id,           -- Name of column having feature vector Ids in train data table
    label_column_name,  -- Name of column having actual label/vlaue for corresponding feature vector in training data table
    test_source,        -- Test data table having features as vector column. Id of features is mandatory
    test_column_name,   -- Name of column having feature vectors in test data table
    test_id,     -- Name of column having feature vector Ids in test data table
    output_table,       -- Name of output table
    k,                  -- value of k. Default will go as 1
    output_neighbors    -- Outputs the list of k-nearest neighbors that were used in the voting/averaging.
    fn_dist             -- The name of the function to use to calculate the distance from a data point to a centroid.
    weighted_avg         Calculates the Regression or classication of k-NN using the weighted average method.
    );

-----------------------------------------------------------------------
                            OUTPUT
-----------------------------------------------------------------------
The output of the KNN module is a table with the following columns:

id                  The ids of test data points.
test_column_name    The test data points.
prediction          The output of KNN- label in case of classification, average value in case of regression.
k_nearest_neighbours The list of k-nearest neighbors that were used in the voting/averaging.
"""
    else:
        help_string = """
----------------------------------------------------------------------------
                                SUMMARY
----------------------------------------------------------------------------
k-Nearest Neighbors is a method for finding k closest points to a given data
point in terms of a given metric. Its input consist of data points as features
from testing examples. For a given k, it looks for k closest points in
training set for each of the data points in test set. Algorithm generates one
output per testing example. The output of KNN depends on the type of task:
For Classification, the output is majority vote of the classes of the k
nearest data points. The testing example gets assigned the most popular class
among nearest neighbors. For Regression, the output is average of the values
of k nearest neighbors of the given testing example.
--
For an overview on usage, run:
SELECT {schema_madlib}.knn('usage');
"""

    return help_string.format(schema_madlib=schema_madlib)
# ------------------------------------------------------------------------------
